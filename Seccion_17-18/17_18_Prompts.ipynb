{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! pip install langchain langchain-core --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv()) # Lee el archivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Sensibilidad de un prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Comprobamos cuan sensible es un LLM al prompt\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "gemma1 = ChatGroq(model=\"gemma-7b-it\",\n",
    "                   temperature=0,\n",
    "                   max_tokens=50)\n",
    "messages = [\n",
    "    SystemMessage(content=\"Eres un esquimal de Groenlandia.\"),\n",
    "    HumanMessage(content=\"\"\"Te gusta el helado? Debes responder si o no. \n",
    "        \"\"\"),             # Agregar unicamente\n",
    "]\n",
    "print(gemma1.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plantilla PromptTemplate\n",
    "Propósito del ejemplo  <br>\n",
    "Mostrar cómo utilizar la plantilla PromptTemplate empleando PromptTemplate como constructor directo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables dinámicas y el método cadena f de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Empleando el metodo cadena f\n",
    "variable1 = \"Buenos dias\"\n",
    "variable2 = \"Gustavo\"\n",
    "print(f\"{variable1}, como esta usted? Me llamo {variable2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Empleando el constructor PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"{saludo}, cómo está usted? Me llamo {nombre}\",\n",
    "    input_variables=[\"saludo\", \"nombre\"],  )\n",
    "\n",
    "prompt=prompt_template.format(saludo=\"Buenos dias\", nombre=\"Gustavo\")\n",
    "print(prompt)\n",
    "print(type(prompt))\n",
    "print(type(prompt_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parámetro template fuera de la plantilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader =WebBaseLoader(web_path=\"https://www.paradigmadigital.com/dev/soluciones-streaming-kafka\")\n",
    "doc2 = loader.load()[0]\n",
    "\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\",\n",
    "                   temperature=1,\n",
    "                   max_tokens=200)\n",
    "\n",
    "template_resumen = \\\n",
    "\"\"\"\n",
    "Quiero un resumen del texto que se encuentra entre triples comillas simples.\n",
    "El resumen no debe superar  las {num_palabras} palabras.\n",
    "Muestra las ideas principales en formato de bullets.\n",
    "Como maximo usa {num_bullets} bullet points.\n",
    "'''{input}'''\n",
    "\"\"\"\n",
    "# Construir la plantilla\n",
    "prompt_template = PromptTemplate(template=template_resumen, \n",
    "                input_variables=[\"input','num_palabras\",\"num_bullets\"])\n",
    "\n",
    "# Asignar valores\n",
    "messages = prompt_template.format(input=doc2.page_content, \n",
    "                                  num_palabras=300, num_bullets=3)\n",
    "# Invocar el modelo\n",
    "res = llm.invoke(input=messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Empleando el método from_template\n",
    "Propósito del ejemplo  <br>\n",
    "Mostrar cómo utilizar la plantilla PromptTemplate empleando el método from_template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt_1 = PromptTemplate.from_template(\"Estoy por viajar a {ciudad}. \")\n",
    "intercala = \" Por favor, \"\n",
    "prompt_2 = PromptTemplate.from_template(\"dígame qué hacer en {tiempo_de_estadia}.\")\n",
    "\n",
    "prompt = (prompt_1 + intercala + prompt_2)\n",
    "prompt.format(ciudad=\"Singapur\", tiempo_de_estadia=\"un mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modular el comportamiento del modelo\n",
    "Propósito del ejemplo <br>\n",
    "Introducir el uso de un template más estructurado que los vistos anteriormente y mostrar cómo el prompt puede condicionar el comportamiento de un LLM. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\" \n",
    "Responde la consulta de acuerdo con el contexto que se da en Contexto.\n",
    "###\n",
    "Contexto: Eres un genetista que estudia la evolución de las especies y estás \\\n",
    "particularmente interesado en los cambios anatómicos con el tiempo.\n",
    "###\n",
    "Pregunta: \"Cuantos dedos tiene un {especie1}, {especie2}, y {especie3}?\"\n",
    "###\n",
    "Respuesta: \n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)    \n",
    "prompt = prompt_template.format(especie1=\"elefante\", especie2=\"lagarto\", especie3=\"ser humano\") # lagarto, ser humano, elefante\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\",\n",
    "               temperature=0,\n",
    "               max_tokens=300)\n",
    "\n",
    "print(llm.invoke(prompt).content)\n",
    "print(\"**********************************************\")\n",
    "#print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guardar un prompt en el disco duro\n",
    "Propósito del ejemplo <br>\n",
    "Mostrar cómo guardar una plantilla de prompt y recuperarla para otro uso. Se almacena en formato JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt_template.save(\"./directorio/mi_prompt.json\") # Guardar en archivo JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Y recuperarlo con\n",
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "recupera_prompt = load_prompt(\"./directorio/mi_prompt.json\")\n",
    "\n",
    "assert prompt_template == recupera_prompt\n",
    "print(recupera_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 1: Resumen de texto\n",
    "Propósito del ejemplo <br>\n",
    "Implementar un código que resume un texto ajustándose a instrucciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "%INSTRUCCIONES:\n",
    "Por favor devuelva un resumen del texto adjunto empleando menos de 100 palabras.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "texto = open(\"directorio/ConstitucionArgentina.txt\", encoding='utf-8').read()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt_final = prompt.format(text=texto)\n",
    "#print(prompt_final)\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\",\n",
    "              temperature=0.25,\n",
    "              max_tokens=150)\n",
    "\n",
    "salida = llm.invoke(prompt_final)\n",
    "print(salida.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plantilla FewShotPromptTemplate - Ejemplo 1\n",
    "Propósito del ejemplo  <br>\n",
    "Implementar una plantilla FewShotPromptTemplate. Documentar el uso de PromptTemplate y los atributos prefix y suffix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "mis_ejemplos = [\n",
    "    {\"pregunta\": \"La capital de Perú es:\", \"respuesta\": \"Lima\"},\n",
    "    {\"pregunta\": \"El autor de El Quijote es:\", \"respuesta\": \"Cervantes\"},\n",
    "]\n",
    "template_modelo = \"\"\"\n",
    "Usuario: {pregunta}\n",
    "Asistente: {respuesta} \"\"\"\n",
    "\n",
    "# Se crea la plantilla definiendo las variables dinámicas y asignando example_format a la plantilla utilizando PromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    template=template_modelo,  #\"Pregunta: {pregunta}\\n{respuesta}\"\n",
    "    input_variables=[\"pregunta\", \"respuesta\"],\n",
    ")\n",
    "print(example_prompt.format(**mis_ejemplos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigue la implementación de la plantilla FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "prefix=(\"Sigue el estilo de respuesta que se muestra en mis_ejemplos. \"\n",
    "            \"La respuesta debe limitarse a unas pocas palabras.\\n\"\n",
    "            \"Preferentemente solo el nombre de la localidad o de la persona.\")\n",
    "# Se instancia un objeto de la clase `FewShotPromptTemplate`  \n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # El prefix es una instrucción para que el LLM sepa que debe hacer con estos ejemplos\n",
    "    prefix=prefix,    \n",
    "    \n",
    "    # le pasamos los ejemplos\n",
    "    examples=mis_ejemplos,\n",
    "    \n",
    "    # le pasamos la plantilla\n",
    "    example_prompt=example_prompt,\n",
    "\n",
    "    # El suffix es donde se agrega el input del usuario para generar el texto que sigue\n",
    "    suffix= \"Responde esta pregunta: {input}\" ,\n",
    "    \n",
    "    # indicamos la variable que se agregará al template\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se concluye la construcción del prompt con format, que asigna valor a input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generamos el prompt con la función format()\n",
    "entrada = few_shot_prompt.format(input=\"El automovilista argentino mas famoso es:\")\n",
    "print(entrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "llm.invoke(entrada).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FewShotPromptTemplate - Ejemplo 2\n",
    "Propósito del ejemplo <br>\n",
    "Mostrar cómo con ejemplos se orienta al modelo para que distinga el significado de una palabra en base al contexto. Se trata de lograr que asiento, del verbo asentir, no se confunda con silla o banco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Inicializar el modelo\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ejemplos = [{\"input\":\"asiento\", \"output\":\"afirmo\"},\n",
    "           {\"input\":\"asiento\", \"output\":\"consiento\"},\n",
    "           {\"input\":\"asiento\", \"output\":\"digo que si\"},]\n",
    "ej_prompt = PromptTemplate.from_template(\n",
    "    \"Palabra:{input}\\nExplicacion:{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=ejemplos,\n",
    "    example_prompt=ej_prompt,\n",
    "    prefix=\"Responda en español.\",\n",
    "    suffix=\"Palabra:{input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "llm.invoke(prompt.format(input=\"asiento\")).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plantilla ChatPromptTemplate\n",
    "Propósito del ejemplo <br>\n",
    "El ejemplo emplea la clase ChatPromptTemplate y el método from_messages para incorporar la lista de mensajes a chat_template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Eres un experto guía de turismo internacional.\"),\n",
    "        (\"human\", \"Buen día. Planifico un viaje y necesito asesoramiento.\"),\n",
    "        (\"ai\", \"Por supuesto que le ayudaré! A dónde viaja?\"),\n",
    "        (\"human\", \"{destino}\"),\n",
    "        (\"ai\", \"Buena opción! Tengo varias recomendaciones para {destino}.\"),\n",
    "        (\"human\", \"Genial cuénteme.\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Se construye el prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format_messages devuelve tipo:  <class 'list'>\n",
      "*******************************************************\n",
      "content='Por supuesto que le ayudaré! A dónde viaja?'\n"
     ]
    }
   ],
   "source": [
    "# Con format_messages devuelve una lista\n",
    "messages2 = chat_template.format_messages(destino=\"Paris\")\n",
    "print(\"format_messages devuelve tipo: \", type(messages2))\n",
    "print(\"*******************************************************\")\n",
    "print(messages2[2])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format_prompt devuelve tipo:  <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "*******************************************************\n",
      "System: Eres un experto guía de turismo internacional.\n",
      "Human: Buen día. Planifico un viaje y necesito asesoramiento.\n",
      "AI: Por supuesto que le ayudaré! A dónde viaja?\n",
      "Human: Paris\n",
      "AI: Buena opción! Tengo varias recomendaciones para Paris.\n",
      "Human: Genial cuénteme.\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "# Con format_prompt devuelve un PromptValue\n",
    "messages1 = chat_template.format_prompt(destino=\"Paris\") \n",
    "print(\"format_prompt devuelve tipo: \", type(messages1))\n",
    "print(\"*******************************************************\")\n",
    "print(messages1.to_string())   # metodos to_string y to_messages\n",
    "print(\"*******************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con format_prompt:  Paris, la ciudad de la luz, es una de las más románticas y fascinantes del mundo. Hay tantas cosas que ver y hacer allí que no sé por dónde empezar.\n",
      "\n",
      " Primero, hay que mencionar los lugares emblemáticos como la Torre Eiffel, el Arco de Triunfo, la Catedral de Notre-Dame y el Museo Louvre, donde se encuentra la Mona Lisa. Pero también hay muchos lugares más ocultos y emocionantes, como el Jardín de Luxemburgo, el Panteón o el Mercado de Puerta de San Jacques.\n",
      "\n",
      "Además, Paris es conocida por sus calles empedradas y sus cafés, como el Café de Flore o el Les Deux Magots, donde puede disfrutar de un café o un desayuno como los parisienses.\n",
      "\n",
      "Y no podemos olvidar la moda y el arte, con diseñadores como Chanel, Dior y Yves Saint Laurent, y museos como el Musée Rodin o el Musée d'Orsay, que albergan obras de artistas como Monet, Renoir y Van Gogh.\n",
      "\n",
      "También hay muchas opciones para disfrutar de la noche parisiense, como el Barrio Latino, el Quartier Latin o el Marais, donde puede encontrar bares, discotecas y teatros.\n",
      "\n",
      "¿Qué tipo de viaje está planeando? ¿Es un viaje romántico, de negocios o de aventura?\n",
      "**************************************\n",
      "Con format_messages:  ¡Claro! Paris, la \"Ciudad de la Luz\", es una de las ciudades más románticas y visitadas del mundo. Hay tanto que ver y hacer allí. Aquí te presento algunas sugerencias:\n",
      "\n",
      "**Monumentos y lugares emblemáticos**:\n",
      "\n",
      "1. La Torre Eiffel: La torre más famosa del mundo, con vistas impresionantes de la ciudad.\n",
      "2. La Basilique du Sacré-Cœur: Una iglesia católica en Montmartre, con vistas panorámicas de la ciudad.\n",
      "3. El Arco del Triunfo: Un monumento dedicado a lossoldados franceses que murieron en la guerra.\n",
      "4. El Louvre: Un museo de arte impresionante, con obras maestras como la Mona Lisa.\n",
      "5. Notre-Dame de París: La catedral gótica más famosa de Francia, aunque actualmente en restauración después del incendio de 2019.\n",
      "\n",
      "**Barrios y distritos**:\n",
      "\n",
      "1. Montmartre: Un barrio artístico y bohemio, con calles empedradas y vistas impresionantes.\n",
      "2. Le Marais: Un barrio trendy y cultural, con galerías de arte y restaurantes.\n",
      "3. Champs-Élysées: El paseo más famoso de París, con tiendas y restaurantes.\n",
      "4. Latin Quarter: Un barrio universitario y bohemio, con calles empedradas y bares.\n",
      "\n",
      "**Rutas y excursiones**:\n",
      "\n",
      "1. Un paseo por el Sena: Un paseo en barca por el río Sena, con vistas de la ciudad.\n",
      "2. Un tour de los museos: Un recorrido por los mejores museos de París, como el Louvre, Orsay y Rodin.\n",
      "3. Un día en Disneyland Paris: Un parque temático para los más pequeños.\n",
      "4. Un paseo por los jardines de París: Un paseo por los jardines más hermosos de la ciudad, como el Jardín de las Tullerías.\n",
      "\n",
      "**Comida y bebida**:\n",
      "\n",
      "1. Un café en un café parisino: Un café en un café como Café de Flore o Les Deux Magots.\n",
      "2. Un paseo por el Mercado de San Miguel: Un mercado de alimentos y bebidas con opciones para todos.\n",
      "3. Un plat du jour: Un plato del día en un restaurante tradicional francés.\n",
      "4. Un vino en un bistró: Un vino en un bistró como Le Comptoir du Relais.\n",
      "\n",
      "Estas son solo algunas de las muchas opciones que tiene en París. ¿Qué te gustaría hacer o ver en particular?\n"
     ]
    }
   ],
   "source": [
    "# Se pasa el prompt al modelo\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(model=\"llama3-8b-8192\")\n",
    "res1 = chat.invoke(messages1)  \n",
    "print(\"Con format_prompt: \", res1.content)\n",
    "print(\"**************************************\")\n",
    "res2 = chat.invoke(messages2)\n",
    "print(\"Con format_messages: \", res2.content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variable única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Eres un experto guía de turismo internacional.\"),\n",
    "        (\"human\", \"Buen día. Planifico un viaje y necesito asesoramiento.\"),\n",
    "        (\"ai\", \"Por supuesto que le ayudaré! A dónde viaja?\"),\n",
    "        (\"human\", \"Viajo a {destino}\"),\n",
    "        (\"ai\", \"Buena opción! Tengo varias recomendaciones para {destino}.\"),\n",
    "        (\"human\",\"Gracias!\")\n",
    "    ]\n",
    ")\n",
    "prompt = chat_template.invoke(\"Roma\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables múltiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Eres un experto guía de turismo internacional.\"),\n",
    "        (\"human\", \"Buen día. Planifico un viaje y necesito asesoramiento.\"),\n",
    "        (\"ai\", \"Por supuesto que le ayudaré! A dónde viaja?\"),\n",
    "        (\"human\", \"Viajo a dos ciudades {destino1} y {destino2}\"),\n",
    "        (\"ai\", \"Buena opción! Tengo varias recomendaciones tanto para {destino1} como para {destino2}.\"),\n",
    "        (\"human\",\"Gracias!\")\n",
    "    ]\n",
    ")\n",
    "prompt = chat_template.invoke({\"destino1\":\"Roma\", \"destino2\":\"Paris\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "##### Empleando las clases SystemMessage, HumanMessage y AIMessage\n",
    "Propósito del ejemplo <br>\n",
    "Las respuestas previas muestran que internamente los roles human, system y ai en las tuplas han sido remplazados por las clases HumanMessage, SystemMessage y AIMessage, respectivamente, cada una con un parámetro content. Esto sugiere que se pueden emplear las clases correspondientes para construir los mensajes, como se ve en este ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Sigue la respuesta del modelo. ****\n",
      "De hecho, París es una ciudad con una gran cantidad de lugares históricos, culturales y de entretenimiento. ¿Qué tipo de experiencia está buscando? ¿Le gustaría visitar lugares emblemáticos como la Torre Eiffel, el Louvre o Notre-Dame? ¿O prefiere explorar barrios como Montmartre, Le Marais o Saint-Germain-des-Prés? ¿Tiene algún interés en particular, como la moda, el arte o la gastronomía?\n",
      "\n",
      "También puedo recomendarte algunos consejos prácticos para planificar su viaje, como la mejor época para visitar, cómo llegar al aeropuerto, dónde aloj\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"Eres un experto guía de turismo internacional.\"),\n",
    "        HumanMessage(content= \"Buen día. Planifico un viaje y necesito asesoramiento.\"),\n",
    "        AIMessage(content= \"Por supuesto que le ayudaré! A dónde viaja?\"),\n",
    "        HumanMessage(content=  \"Deseo viajar a Paris\"),\n",
    "        AIMessage(content=\"Buena opción! Tengo varias recomendaciones para ese destino.\"),\n",
    "        HumanMessage(content= \"Gracias!\")\n",
    "    ]\n",
    ")\n",
    "prompt_final = chat_template.format_messages()\n",
    "print()\n",
    "print(\"**** Sigue la respuesta del modelo. ****\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "modelo_chat = ChatGroq(model=\"llama3-8b-8192\",\n",
    "               temperature=0.25,\n",
    "               max_tokens=150)\n",
    "res = modelo_chat.invoke(prompt_final)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plantilla MessagePromptTemplate\n",
    "Propósito del ejemplo  <br>\n",
    "Implementar una plantilla MessagePromptTemplate combinando las clases HumanMessage y HumanMessagePromptTemplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soixante-dix balcons et pas une fleur.\n",
      "\n",
      "(Note: \"Setenta\" is translated to \"soixante-dix\" in French, as \"setenta\" is the Spanish word for \"seventy\", and \"balcones\" is translated to \"balcons\". \"Ni\" is translated to \"pas\", which is a more common way to express \"no\" or \"none\" in French. Finally, \"flor\" is translated to \"fleur\".)\n"
     ]
    }
   ],
   "source": [
    "texto = \"Setenta balcones y ninguna flor.\"\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "     \"Eres un experto traductor multilingüe; por favor traduce del {lenguaje_original} al {traducir_al}.\"),\n",
    "    HumanMessage(content=\"Ayudame a traducir este texto.\"),\n",
    "    AIMessage(content=\"Por favor provea el texto\"),   \n",
    "    HumanMessagePromptTemplate.from_template(texto)\n",
    "   ])\n",
    "# Se aplica el método format_messages de ChatPromptTemplate. Devuelve mensajes.\n",
    "prompt_final = template.format_messages(lenguaje_original=\"español\", traducir_al=\"francés\", texto=texto)\n",
    "# Se pasa el prompt al modelo\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(model=\"llama3-8b-8192\",\n",
    "               temperature=0.25,\n",
    "               max_tokens=150)   # Se instancia un objeto ChatGroq\n",
    "res = chat.invoke(prompt_final)  # Se pasa el prompt al modelo\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain-of-Thought 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Vamos a resolver esto paso a paso!\n",
      "\n",
      "1. Primero, necesitamos encontrar cuántos centímetros de cinta tenemos en total: 120 cm.\n",
      "2. Luego, necesitamos encontrar cuántos trozos de 15 cm de largo podemos cortar de la cinta. Para hacer esto, podemos dividir el total de centímetros de cinta (120 cm) entre la longitud de cada trozo (15 cm).\n",
      "3. Para dividir, podemos utilizar la regla de tres: 120 cm ÷ 15 cm = ?\n",
      "4. Al dividir, obtenemos: 120 ÷ 15 = 8\n",
      "\n",
      "¡Eso significa que podemos cortar 8 trozos de 15 cm de largo de la cinta!\n",
      "\n",
      "Así que la respuesta es: 8 trozos de 15 cm de largo.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "template = \"\"\"\n",
    "Tengo una cinta de 120 cm de largo. Debo cortarla en trozos de 15 cm de largo cada uno. \n",
    "Cuantos trozos de 15 cm de largo puedo obtener?\n",
    "Piense paso a paso.\n",
    "\"\"\"\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\",\n",
    "               temperature=0.1,\n",
    "               max_tokens=1000,\n",
    "               verbose=True)   # Se instancia un objeto ChatGroq\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[])\n",
    "prompt_final = prompt.format()\n",
    "\n",
    "\n",
    "# Procesar con un LLM\n",
    "response = llm.invoke(prompt_final)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain-of-Thought  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excelente problema! Vamos a resolverlo paso a paso.\n",
      "\n",
      "**Lista de subproblemas:**\n",
      "\n",
      "1. ¿Quién es el escritor argentino famoso por sus cuentos cortos?\n",
      "2. ¿Por qué sus obras fueron prohibidas por la última dictadura militar en Argentina?\n",
      "3. ¿Dónde se exilió el escritor?\n",
      "4. ¿Cuándo falleció el escritor?\n",
      "5. ¿Dónde pasó sus últimos años de vida y dónde falleció?\n",
      "\n",
      "**Resolución de los subproblemas:**\n",
      "\n",
      "1. ¿Quién es el escritor argentino famoso por sus cuentos cortos?\n",
      "\n",
      "No hay mucha información disponible sobre este escritor, pero podemos buscar en la literatura argentina y ver quién se ajusta a los demás subproblemas.\n",
      "\n",
      "2. ¿Por qué sus obras fueron prohibidas por la última dictadura militar en Argentina?\n",
      "\n",
      "La última dictadura militar en Argentina fue la dictadura autodenominada \"Proceso de Reorganización Nacional\", que gobernó el país desde 1976 hasta 1983. Es probable que las obras del escritor fueran prohibidas debido a su contenido político o social crítico.\n",
      "\n",
      "3. ¿Dónde se exilió el escritor?\n",
      "\n",
      "La dictadura militar en Argentina fue conocida por su represión política y su persecución de intelectuales y artistas. Es probable que el escritor se exilió a un país con una tradición democrática y respetuosa con los derechos humanos.\n",
      "\n",
      "4. ¿Cuándo falleció el escritor?\n",
      "\n",
      "No hay mucha información disponible sobre la fecha de fallecimiento del escritor, pero podemos buscar en la literatura argentina y ver quién se ajusta a los demás subproblemas.\n",
      "\n",
      "5. ¿Dónde pasó sus últimos años de vida y dónde falleció?\n",
      "\n",
      "Es probable que el escritor pasara sus últimos años de vida en un país donde se exilió y donde se sintió seguro y respetado.\n",
      "\n",
      "**Respuesta final:**\n",
      "\n",
      "Después de investigar y analizar los subproblemas, creo que el escritor argentino famoso por sus cuentos cortos que fue prohibido por la última dictadura militar en Argentina y se exilió a Francia es Jorge Luis Borges. Borges fue un escritor argentino famoso por sus cuentos cortos y ensayos filosóficos, y sus obras fueron prohibidas por la dictadura militar en Argentina. Se exilió a Francia en 1977 y falleció en París en 1986.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "problema = \"\"\"\n",
    "Deseo recordar el nombre de un escritor argentino famoso por sus cuentos cortos. \\\n",
    "Sus obras fueron prohibidas por la última dictadura militar en Argentina. Se exilió a Francia. Ya falleció. \\\n",
    "Sus últimos años de vida transcurrieron en París, donde falleció.\"\"\"\n",
    "\n",
    "instrucciones = \"\"\"Planteo del problema: \n",
    "\n",
    "Primero, haga una lista de los subproblemas que plantea este problema y que deben ser resueltos para llegar a la respuesta correcta.\n",
    "Segundo, resuelva cada subproblema apoyándose en las soluciones previas.\n",
    "Tercero, muestre la respuesta final.\n",
    "Por favor siga las instrucciones para un buen resultado.\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "Problema:\n",
    "{problema}\n",
    "\n",
    "Instrucciones:\n",
    "{instrucciones}\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\",\n",
    "               temperature=0.1,\n",
    "               max_tokens=1000,\n",
    "               verbose=True)   # Se instancia un objeto ChatGroq\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['problema', 'instrucciones'])\n",
    "prompt_final = prompt.format(problema=problema, instrucciones=instrucciones)\n",
    "\n",
    "respuesta = llm.invoke(prompt_final)\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
