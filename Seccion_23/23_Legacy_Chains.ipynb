{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7524ea2c-a3db-48b8-979a-bae61a0615e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd583c17-f400-49f2-ae49-54f47d379284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74237e1b-e964-40bf-86e1-d93227ce1096",
   "metadata": {},
   "source": [
    "### Adecuación de Legacy Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8dd34-62d9-4057-a96a-bfb502f7c495",
   "metadata": {},
   "source": [
    "##### Cadenas con LLMChain (legacy)\n",
    "Propósito del ejemplo <br>\n",
    "Mostrar cómo adecuar el constructor LLMChain a LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ab69d-5472-41a1-baa7-18a1fcdb5614",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Codigo base\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "mixtral = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "# creamos el template\n",
    "prompt = ChatPromptTemplate.from_template(\"Hola cómo estás? mi nombre es {name}\",)\n",
    "\n",
    "# Implementacion de LLMChain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=mixtral)\n",
    "respuesta1 = chain.invoke(input=\"Fernando\")\n",
    "respuesta1[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07fe71-2ab1-46a6-b9de-73516ac9a10f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ec44e-0d5f-4ddc-af39-3054ca712372",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1791324-238a-49c9-9212-49830fed413c",
   "metadata": {},
   "source": [
    "#####  Remplazo con RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa627e0-7258-4785-96b8-202ba79cc191",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola Fernando, ¡mucho gusto en conocerte! Estoy aquí para ayudarte, ¿en qué puedo assistirte hoy?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "chain = RunnableSequence(first=prompt, last=mixtral )\n",
    "respuesta3 = chain.invoke(\"Fernando\")\n",
    "respuesta3.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cbb4d-f600-4df4-8ddb-1ce5c0f01837",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fba3ee-c789-43d4-b25e-c76147d059eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fdd5882-abbf-427c-a56e-3daa1014b616",
   "metadata": {},
   "source": [
    "##### Remplazo con operador tubo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c32c8b-62e4-45aa-9e79-d3af6a783bed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola Fernando, ¡qué tal estás tú? Encantado de saludarte. Si necesitas ayuda con algo, no dudes en preguntar. Estoy aquí para asistirte en lo que necesites. ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain =   prompt |  mixtral\n",
    "respuesta2 = chain.invoke(input=\"Fernando\")\n",
    "respuesta2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae76cfe-d268-435e-abf5-f2b69abed553",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c11a2-3e59-48ff-9d39-e2d99108abdc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e44b94-ca03-4929-9b5f-6c330ba0871d",
   "metadata": {},
   "source": [
    "#### Cadenas con RetrievalQA\n",
    "Propósito del ejemplo <br>\n",
    "RetrievalQA es un constructor de cadenas de uso común que ha sido clasificado como obsoleto. Este ejemplo muestra cómo adecuarlo a una sintaxis LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb46e5b3-2cf5-4d30-82c1-ee7092580943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous agents are systems that use a large language model (LLM) as their core controller for problem-solving. These agents consist of several components, including planning and task decomposition. The LLM functions as the agent's brain, guiding its decision-making process. Additionally, autonomous agents often employ self-reflection to learn from past actions, improve decision-making, and rectify errors.\n",
      "<class 'dict'>\n",
      "********************************************\n",
      "Autonomous agents are systems that use a large language model (LLM) as their core controller to perform tasks. They can learn to call external APIs for additional information and improve iteratively through self-reflection, which helps them refine past action decisions and correct previous mistakes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "loader = TextLoader('directorio/LLMPoweredAutonomousAgents.txt')\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=HuggingFaceBgeEmbeddings())\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Implementacion de RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Documentacion del prompt en https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "qa_chain1 = RetrievalQA.from_llm(\n",
    "    llm, retriever=vectorstore.as_retriever(), prompt=prompt)\n",
    "\n",
    "respuesta = qa_chain1.invoke(\"What are autonomous agents?\")\n",
    "print(StrOutputParser().parse(respuesta['result']))\n",
    "print(type(respuesta))\n",
    "print(\"********************************************\")\n",
    "\n",
    "# El remplazo con sintaxis de LCEL\n",
    "from langchain_core.runnables import (RunnablePassthrough)\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain2 = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "respuesta = qa_chain2.invoke(\"What are autonomous agents?\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73085e-04cf-41ef-a067-766e9051caf1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca05c4c-67c0-4909-bd68-bf5f5f99fbd6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90aba0-e383-4114-9593-212838a1e377",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a95c21-92c4-4001-9e6e-a70a9f96a37a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ad08c-ce07-466d-a70b-2efa7c6f48fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43bb39cf-6d9f-4b9b-83e5-f6f4ad220ae2",
   "metadata": {},
   "source": [
    "##### Remplazo con create_stuff_documents_chain y create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8fc90ff-8b47-46f4-91ce-c8bc7ffad5cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Autonomous agents are systems that can make decisions and perform tasks independently without direct human intervention. In the provided context, autonomous agents are powered by a neuro-symbolic architecture called MRKL, which consists of various \"expert\" modules that can be neural or symbolic. The LLM (likely a language model) acts as a router to direct inquiries to the most suitable expert module. These agents have a planning component that helps them break down complex tasks into smaller steps through a process called task decomposition.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Documentacion del prompt en https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "\n",
    "rag_chain.invoke({\"input\": \"What are autonomous agents?\"})[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bd593-997a-4f3d-a0a3-861649dc88e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a8076-5f4b-4044-8571-8e58c5803d2a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize your retriever (e.g., using Chroma)\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Initialize your LLM\n",
    "llm = OpenAI()\n",
    "\n",
    "# Load a prompt from LangChain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define the LCEL chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Use the chain\n",
    "response = rag_chain.invoke(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
