{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificar ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# El servicio de Groq se instala con el paquete de integración langchain-groq.\n",
    "!pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "# LangChain facilita interactuar con diferentes LLM\n",
    "llama3 = ChatGroq(model=\"llama3-8b-8192\") # Meta\n",
    "mixtral = ChatGroq(model=\"mixtral-8x7b-32768\") # Mistral\n",
    "gemma = ChatGroq(model=\"gemma-7b-it\") # Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mensajes como tuplas\n",
    "Propósito del ejemplo  <br>\n",
    "Demostrar la implementación de un modelo chat con mensajes en formato de tupla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Hola Alberto! ¡Bienvenido! Me alegra poder ayudarte. ¿En qué puedo asistirte? ¿Estás planeando un viaje y necesitas recomendaciones o tienes alguna pregunta sobre un destino en particular? Estoy aquí para ayudarte en lo que necesites. ¡Hablemos!' response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 69, 'total_tokens': 134, 'completion_time': 0.054166667, 'prompt_time': 0.00824807, 'queue_time': 0.006398209, 'total_time': 0.062414737}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-a92bad27-f567-442c-a97a-f998c2c40620-0' usage_metadata={'input_tokens': 69, 'output_tokens': 65, 'total_tokens': 134}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llama3 = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "template = [\n",
    "    (\"system\", \"Eres un chatbot guia de turismo simpatico y servicial. Tu nombre es Juan.\"),\n",
    "    (\"human\", \"Hola, cómo estás Juan?\"),\n",
    "    (\"ai\", \"Muy bien, gracias!\"),\n",
    "    (\"human\", \"Me llamo Alberto y necesito ayuda.\"),\n",
    "]\n",
    "respuesta = llama3.invoke(template)     # remplaza .run\n",
    "print(type(respuesta))\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Alberto! ¡Bienvenido! Me alegra poder ayudarte. ¿En qué puedo asistirte? ¿Estás planeando un viaje y necesitas recomendaciones o tienes alguna pregunta sobre un destino en particular? Estoy aquí para ayudarte en lo que necesites. ¡Hablemos!\n"
     ]
    }
   ],
   "source": [
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_usage': {'completion_tokens': 31, 'prompt_tokens': 71, 'total_tokens': 102, 'completion_time': 0.025833333, 'prompt_time': 0.013415764, 'queue_time': 0.925996554, 'total_time': 0.039249097}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "print(respuesta.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Clases SystemMessage, HumanMessage, AIMessage\n",
    "Propósito del ejemplo  <br>\n",
    "Demostrar la implementación de un modelo chat con mensajes empleando clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido: ¡Fantástico! París es un paraíso para los amantes de los museos. Hay muchos que elegir, dependiendo de sus intereses. Algunos de los más famosos son:\n",
      "\n",
      "**Para los amantes de la pintura:**\n",
      "\n",
      "* **Museo del Louvre:** Contiene obras maestras de artistas como Leonardo da Vinci, Monet, Van Gogh y Rembrandt.\n",
      "* **Musée d'Orsay:** Famoso por sus pinturas impresionistas y post-impresionistas.\n",
      "* **Musée Picasso:** Contiene la colección más completa de obras del artista español Pablo Picasso.\n",
      "\n",
      "**Para los amantes de la historia:**\n",
      "\n",
      "* **Museo Nacional del Castillo de Versalles:** Es el palacio real más grande de Europa.\n",
      "* **Museo Carnavalet:** Contiene objetos históricos relacionados con la historia de París.\n",
      "* **Museo de l'Histoire de Paris:** Contiene la historia de la ciudad de París.\n",
      "\n",
      "**Para los amantes de la cultura:**\n",
      "\n",
      "* **Musée Rodin:** Contiene la colección completa de esculturas del famoso escultor francés Auguste Rodin.\n",
      "* **Musée d'Art et d'Histoire:** Contiene una mezcla de arte y objetos históricos.\n",
      "* **Centre Pompidou:** Centro de arte moderno y contemporáneo.\n",
      "\n",
      "Además de estos museos, también hay muchos otros lugares históricos y culturales que puede visitar en París. Algunos de ellos son:\n",
      "\n",
      "* **La Torre Eiffel:** Una de las estructuras más emblemáticas de la ciudad.\n",
      "* **El Palacio del Louvre:** El palacio real más grande de Europa.\n",
      "* **El barrio de Montmartre:** Famoso por sus calles empedradas y sus vistas panorámicas de la ciudad.\n",
      "* **La Plaza de la Concorde:** Una plaza histórica con una rica historia.\n",
      "\n",
      "Para planificar su viaje, le recomiendo que:\n",
      "\n",
      "* **Elige los museos que más se ajustan a sus intereses.**\n",
      "* **Comprete la \"Paris Pass\" o la \"Museum Pass\" para obtener descuentos en la entrada a los museos.**\n",
      "* **Planifique su itinerario de forma que pueda visitar los museos y otros lugares turísticos sin agotarse.**\n",
      "* **Esté preparado para hacer cola, especialmente en los días de alta temporada.**\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "gemma = ChatGroq(model=\"gemma-7b-it\") # Google\n",
    "\n",
    "messages = [\n",
    "        SystemMessage(content=\"Eres un simpático guía de turismo \\\n",
    "        internacional.\"),\n",
    "        HumanMessage(content= \"Buen día, busco información para planificar \\\n",
    "        una visita a París.\"),\n",
    "        AIMessage(content= \"Con gusto le ayudaré! Tiene algún interés especial?\"),\n",
    "        HumanMessage(content= \"Me interesa visitar los museos\"),\n",
    "    ]\n",
    "\n",
    "# Al extraer el contenido, los caracteres especiales son correctamente interpretados\n",
    "print(f\"Contenido: {gemma.invoke(messages).content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo chat simil LLM de texto puro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Dios le ayuda!\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llama3 = ChatGroq(model=\"llama3-8b-8192\") # Meta\n",
    "prompt = \"Al que madruga ...\"\n",
    "res = llama3.invoke(prompt)\n",
    "print(res.content)\n",
    "print(type(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos chat y memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo siento, pero no sé tu nombre. ¿Puedes decirme cómo te llamas?\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llama3 = ChatGroq(model=\"llama3-8b-8192\")\n",
    "llama3.invoke(input=\"Hola mi nombre es Gustavo\")\n",
    "print(llama3.invoke(input=\"Por favor repite mi nombre.\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memoria efímera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gustavo!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, SystemMessage, HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llama3 = ChatGroq(model=\"llama3-8b-8192\")\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hola mi nombre es Gustavo.\"),\n",
    "    AIMessage(content=\"Como puedo ayudarle?\"),\n",
    "    HumanMessage(content=\"Repite mi nombre.\")\n",
    "]\n",
    "# messages = [\n",
    "#     \"human\",\"Hola mi nombre es Gustavo.\",\n",
    "#     \"ai\",\"Como puedo ayudarle?\",\n",
    "#     \"human\",\"Repite mi nombre.\"]\n",
    "print(llama3.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Parámetros de control\n",
    "Propósito del ejemplo <br>\n",
    "Demostrar el efecto de usar diferentes temperaturas en dos modelos diferentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Llama 3 8b ---\n",
      "Un sinónimo de \"bondadoso\" es \"amable\".\n",
      "\n",
      "\n",
      "--- Llama 3 70b ---\n",
      "Un sinónimo de \"bondadoso\" es \"benévolo\".\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Comparemos llama 3 de 70b parametros con llama 3 de 8b\n",
    "# Probar varios valores de temperatura\n",
    "# Un articulo reciente sugiere que hay minima diferencia \n",
    "#     en el comportamiento del modelo entre 0 y 1. \n",
    "llama3_8b = ChatGroq(model=\"llama3-8b-8192\",\n",
    "                     temperature=0)\n",
    "llama3_70b = ChatGroq(model=\"llama3-70b-8192\",\n",
    "                      temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Eres profesor de literatura\"),  \n",
    "    HumanMessage(content=\"\"\"Escribe un sinonimo de bondadoso.\"\"\"),] \n",
    "\n",
    "print(\"--- Llama 3 8b ---\")\n",
    "res_8b = llama3_8b.invoke(messages)\n",
    "print(res_8b.content)\n",
    "\n",
    "print(\"\\n\\n--- Llama 3 70b ---\")\n",
    "res_70b = llama3_70b.invoke(messages)\n",
    "print(res_70b.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explorar los metadatos\n",
    "Propósito del ejemplo  <br>\n",
    "Explorar la información contenida en los metadatos. Se trabaja con dos versiones del modelo llama3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Llama 3 8b ---\n",
      "{'finish_reason': 'stop',\n",
      " 'logprobs': None,\n",
      " 'model_name': 'llama3-8b-8192',\n",
      " 'system_fingerprint': 'fp_873a560973',\n",
      " 'token_usage': {'completion_time': 0.013333333,\n",
      "                 'completion_tokens': 16,\n",
      "                 'prompt_time': 0.005505892,\n",
      "                 'prompt_tokens': 32,\n",
      "                 'queue_time': 0.009129737,\n",
      "                 'total_time': 0.018839225,\n",
      "                 'total_tokens': 48}}\n",
      "\n",
      "\n",
      "--- Llama 3 70b ---\n",
      "{'finish_reason': 'stop',\n",
      " 'logprobs': None,\n",
      " 'model_name': 'llama3-70b-8192',\n",
      " 'system_fingerprint': 'fp_753a4aecf6',\n",
      " 'token_usage': {'completion_time': 0.053788841,\n",
      "                 'completion_tokens': 17,\n",
      "                 'prompt_time': 0.00225072,\n",
      "                 'prompt_tokens': 32,\n",
      "                 'queue_time': 0.062317265,\n",
      "                 'total_time': 0.056039561,\n",
      "                 'total_tokens': 49}}\n"
     ]
    }
   ],
   "source": [
    "# Utilizar el paquete `pprint` para imprimir de forma mas legible \n",
    "#   el contenido de `response_metadata` que es un objeto JSON\n",
    "import pprint\n",
    "\n",
    "print(\"--- Llama 3 8b ---\")\n",
    "pprint.pprint(res_8b.response_metadata)\n",
    "\n",
    "print(\"\\n\\n--- Llama 3 70b ---\")\n",
    "pprint.pprint(res_70b.response_metadata)\n",
    "\n",
    "# notar que el modelo de 70b tarda aproximadamente 3 veces mas en responder que el de 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El modelo gemma: temperature y max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como soy un modelo lingüístico y no soy un esquimal de Groenlandia en particular, no puedo tener opiniones o gustos. Sin embargo, puedo proporcionar información sobre los esquimas de Groenlandia y sus preferencias.\n",
      "Como soy\n",
      "Como soy un modelo de lenguaje y no soy un esquimal de Groenlandia, no puedo tener preferencias o gustos, incluyendo el gusto por el helado.\n"
     ]
    }
   ],
   "source": [
    "# Comparamos tres instanciaciones de Gemma 7b\n",
    "## variando ambos parametros\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "gemma1 = ChatGroq(model=\"gemma-7b-it\",\n",
    "                   temperature=1,\n",
    "                   max_tokens=50)\n",
    "\n",
    "gemma2 = ChatGroq(model=\"gemma-7b-it\",\n",
    "                   temperature=1,\n",
    "                   max_tokens=2)\n",
    "\n",
    "gemma3 = ChatGroq(model=\"gemma-7b-it\",\n",
    "                   temperature=0,\n",
    "                   max_tokens=50)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Eres un esquimal de Groenlandia.\"),\n",
    "    HumanMessage(content=\"\"\"Te gusta el helado? \"\"\"),\n",
    "]\n",
    "\n",
    "# notar la diferencia en las respuestas y que no es el formato deseado. Que podemos hacer?\n",
    "print(gemma1.invoke(messages).content)\n",
    "print(gemma2.invoke(messages).content)\n",
    "print(gemma3.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
