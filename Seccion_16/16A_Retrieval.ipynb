{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docarray\n",
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers en LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funcionamiento de as_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "\n",
    "    [\"River Plate es un equipo de futbol\", \"A Juan le gusta leer\"],\n",
    "    embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever.get_relevant_documents(\"Que es River Plate?\")\n",
    "retriever.get_relevant_documents(\"Que le gusta a Juan?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertir un vector store a Retriever\n",
    "VectorStoreBackedRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = PyPDFLoader(\"./directorio/Estructuras.pdf\")\n",
    "documents = loader.load()\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")    #\"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Se crea el vector store\n",
    "persist_directory = \"chroma_db\"\n",
    "db = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el numero de chunks a devolver\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3}, search_type=\"similarity\") #mmr\n",
    "retriever.invoke(\"Como crear una lista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el numero de chunks a devolver\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2}, search_type=\"mmr\")\n",
    "retriever.invoke(\"derechos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imponer un umbral de puntaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\"score_threshold\": 0.1}\n",
    ")\n",
    "retriever.invoke(\"derechos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"directorio/Un mundo feliz - ALDOUS HUXLEY.txt\",       # 447000 caracteres\n",
    "               encoding=\"utf-8\")\n",
    "\n",
    "docs = loader.load()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recuperar documentos de origen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Se crea una BD vectorial\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "# Se particionan los chunks menores\n",
    "menor_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=0)\n",
    "\n",
    "# En store se guardan los documentos de origen\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Se instancia el retriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=menor_splitter,\n",
    ")\n",
    "# Agregar los documentos al retriever\n",
    "retriever.add_documents(docs)\n",
    "# Devuelve las claves de los documentos cargados\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"Como es el sistema de gobierno?\")\n",
    "print(sub_docs[0].page_content)\n",
    "print(len(sub_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Como es el sistema de gobierno?\")\n",
    "print(len(retrieved_docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperar los chunks mayores pero no los documentos\n",
    "Hacer particiones mayores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Instanciar el particionador mayor\n",
    "mayor_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "\n",
    "# Instanciar el particionador menor\n",
    "menor_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=0)\n",
    "\n",
    "# Crear el espacio vectorial donde almacenar los chunks menores\n",
    "# Estos chunks son incrustados\n",
    "vectorstore = Chroma(\n",
    "     collection_name=\"split_parents\", \n",
    "     embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Almacen para los chunks mayores en memoria\n",
    "# Estos chunks no son incrustados\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Se instancia el retriever, que hara la mayor parte del trabajo\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=menor_splitter,\n",
    "    parent_splitter=mayor_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agregan los documentos\n",
    "# Tareas:\n",
    "# 1 Agregar los documentos originales\n",
    "# 2 Particionar en chunks mayores y asignar un ID a cada uno\n",
    "# 3 Almacenar los chunks mayores en docstore, sin incrustar\n",
    "# 4 Particionar los chunks mayores en chunks menores\n",
    "# 5 Incrustarlos y almacenarlos en el especio vectorial\n",
    "# 6 Asignar a los chunks menores los IDs de los mayores para mantener la conexion\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 1160 chunks mayores y 1693 chunks menores.\n"
     ]
    }
   ],
   "source": [
    "num_chunks_mayores = len(retriever.docstore.store.items())\n",
    "num_chunks_menores = len(set(retriever.vectorstore.get()['documents']))\n",
    "# Con set() se eliminan chunks duplicados\n",
    "print (f\"Hay {num_chunks_mayores} chunks mayores y {num_chunks_menores} chunks menores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se recuperaron 4 chunks menores relevantes.\n",
      "Este chunk tiene 370 caracteres.\n",
      "El texto es: Desde luego, no hay razón alguna para que el nuevo totalita- \n",
      "rismo se parezca al antiguo. El Gobierno, por medio de porras \n",
      "y piquetes de ejecución, hambre artificialmente provocada, en- \n",
      "carcelamientos en masa y deportación también en masa no es \n",
      "solamente inhumano (a nadie, hoy día, le importa demasiado \n",
      "este hecho); se ha comprobado que es ineficaz, y en una época\n"
     ]
    }
   ],
   "source": [
    "# Recuperar los chunks menores mas relevantes\n",
    "sub_docs = vectorstore.similarity_search(\"Como es el sistema de gobierno?\")\n",
    "print(f'Se recuperaron {len(sub_docs)} chunks menores relevantes.')\n",
    "print(f'Este chunk tiene {len(sub_docs[0].page_content)} caracteres.')\n",
    "print(f'El texto es: {sub_docs[0].page_content}')\n",
    "# El indice identifica el chunk mayor fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{'source': 'directorio/Un mundo feliz - ALDOUS HUXLEY.txt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "852"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuando se invoca el retriever:\n",
    "# 1 Recupera los chunks menores mas relevantes mediante una busqueda por similitud semantica\n",
    "# 2 Recupera los chunks mayores en docstore\n",
    "# 3 Guarda en retrieved_docs los chunks mayores que son fuente de los menores mas relevantes\n",
    "# 4 Construye el contexto a partir de estos.\n",
    "retrieved_docs = retriever.invoke(\"Como es el sistema de gobierno?\")\n",
    "print(len(retrieved_docs))\n",
    "print(retrieved_docs[0].metadata)\n",
    "len(retrieved_docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Invocar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Según el texto, el sistema de gobierno en este mundo no es el tradicional y violento, donde se utilizan la coerción y la represión para mantener el orden. En su lugar, el sistema de gobierno es más avanzado y utiliza la tecnología para condicionar a la población de manera que no puedan hacer otra cosa más que lo que deben hacer.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se instancia un modelo de lenguaje\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "# Se construye el prompt\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Responda en español.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "question = \"Como es el sistema de gobierno?\"\n",
    "mensaje = prompt.format_prompt(context=retrieved_docs, question=question)\n",
    "res = llm.invoke(input=mensaje)\n",
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
